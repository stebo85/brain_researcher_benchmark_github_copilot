# DL-013: Train transformer architecture for MEG sequence modeling

## Task Id

DL-013

## Task Catefory

Deep Learning

## Mode

Full-Stack

## User Prompt

Train transformer architecture for MEG sequence modeling

## Input Data Ref

MNE sample dataset

## Context Block

Apply self-attention across time points to capture long-range temporal dependencies

## Expected Capability

transformer_tool; sequence_modeling

## Acceptance Metrics

sequence_accuracy>lstm; attention_interpretable

## Evidence Required

trained_transformer.pth; attention_patterns.png

